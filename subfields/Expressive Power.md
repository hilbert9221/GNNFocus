- **A Survey on The Expressive Power of Graph Neural Networks.** *Ryoma Sato.* CoRR 2020.
  - resources: [paper](https://arxiv.org/pdf/2003.04078v4)
  - contributions:
    - A timely and detailed survey on the expressive power of GNNs.
    - Bridging two descriptions of the expressive power of GNNs, graph isomorphism tests and combinational optimization problems.
- **Breaking the Limits of Message Passing Graph Neural Networks.** *Muhammet Balcilar, Pierre Héroux, Benoit Gaüzère, Pascal Vasseur, Sébastien Adam, Paul Honeine.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2106.04319), [code](https://github.com/balcilar/gnn-matlang)
  - contributions:
    - Use a so-called [**MATRIX LANGUAGE**](http://adrem.uantwerpen.be/bibrem/pubs/MLonGRaphs.pdf) to characterize the expressive power in GNNs by considering the linear algebra they use, including matrix multiplication and addtition, elementwise multiplication, taking trace, etc.
    - Point out that recording matrix powers help distinguish some graphs indistinguishable by 1st order WL-test, and motivate the design of including infinite number of Laplacian powers by learning a function over the spectrum of the graphs.
    - Provide a memory efficient implementation of the proposed GNNs. However, it seems the dimension of the hidden layer depends on the number of non-zero elements of matrix powers, which varyies over graphs, making the proposed GNNs not inductive. Besides, maybe it not suitable for large graphs, since the adjacency matrices are oversized.
- **From Local Structures to Size Generalization in Graph Neural Networks.** *Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, Haggai Maron.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2010.08853v2)
  - contributions:
    - Recursively define d-patterns to messure the expressive power of GNNs.
    - Prove that GNNs fail to generalize to unseen d-patterns (e.g., trained on small graphs, tested on large graphs with unseen d-patterns)
    - Empirically propose to tackle the size generalization problem with self-supervised learning by defining pre-text tasks.
- **How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks.** *Keyulu Xu, Mozhi Zhang, Jingling Li, Simon S. Du, Ken-ichi Kawarabayashi, Stefanie Jegelka.* ICLR 2021.
  - resources: [paper](https://openreview.net/pdf?id=UH-cmocLJC), [review](https://openreview.net/forum?id=UH-cmocLJC)
  - contributions:
    - Further work to **Xu et al. ICLR 2020**.
    - Studying the generalization error when the support of the testing set is large than that of the training set. (The geoemtry of the training data affects extrapolation.)
    - Showing that an infinitely wide ReLU MLP converges to a linear mapping and extending the results to GNNs.
- **Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective.** *Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, Paul Honeine.* ICLR 2021.
  - resources: [paper](https://openreview.net/pdf?id=-qh0M9XWxnv), [code](https://github.com/balcilar/gnn-spectral-expressive-power), [review](https://openreview.net/forum?id=-qh0M9XWxnv)
  - contributions:
    - Proposing a unified formulation of GNNs in the spectral domain.
    - Defining the expressive power of spectral GNNs as the frequency response and calculating the frequency responses of several well known GNNs.
    - Insightful visualization of the eigenvalues.
- **Characterizing the Expressive Power of Invariant and Equivariant Graph Neural Networks.** *Waïss Azizian, Marc Lelarge.* ICLR 2021.
  - resources: [paper](https://openreview.net/pdf?id=lxHgXYN4bwl), [code](https://github.com/mlelarge/graph_neural_net), [review](https://openreview.net/forum?id=lxHgXYN4bwl)
- **On Graph Neural Networks versus Graph-Augmented MLPs.** *Lei Chen, Zhengdao Chen, Joan Bruna.* ICLR 2021.
  - resources: [paper](https://openreview.net/pdf?id=tiqI7w64JG2), [review](https://openreview.net/forum?id=tiqI7w64JG2)
  - contributions:
    - Identifying some GNNs that disentangle the propagation step and the transformation step as **Graph-augmented MLPs**.
    - Showing that GNNs are more powerful than Graph-augmented MLPs in terms of graph isomorphism test, but the gap vanishes as the number of propagation steps turns to infinite.
    - Studying the expressive power of GNNs at the node level using the concept of rooted graphs. (I don't understand.)
- **Can Graph Neural Networks Count Substructures?** *Zhengdao Chen, Lei Chen, Soledad Villar, Joan Bruna.* NeurIPS 2020.
  - resources: [paper](https://proceedings.neurips.cc/paper/2020/file/75877cb75154206c4e65e76b88a12712-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/75877cb75154206c4e65e76b88a12712-Review.html), [slides](https://cims.nyu.edu/~chenzh/files/GNN_substructures_short_slides.pdf)
  - contributions:
- **How Hard is to Distinguish Graphs with Graph Neural Networks?** *Andreas Loukas.* NeurIPS 2020.
  - resources: [paper](https://proceedings.neurips.cc/paper/2020/file/23685a2431acad7789c1e3d43ea1522c-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/23685a2431acad7789c1e3d43ea1522c-Review.html)
  - contributions:
- **What Can Neural Networks Reason About?** *Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken-ichi Kawarabayashi, Stefanie Jegelka.* ICLR 2020.
  - resources: [paper](https://openreview.net/pdf?id=rJxbJeHFPS), [review](https://openreview.net/forum?id=rJxbJeHFPS)
  - contributions:
    - Pointing out the connection between GNNs and dynamic programming.
    - Proposing *algorithmic alignment* to measure generalization.
    - Deriving generalization bounds of GNNs via Bayesian PAC theory.
- **What Graph Neural Networks Cannot Learn: Depth versus Width.** *Andreas Loukas.* ICLR 2020.
  - resources: [paper](https://openreview.net/pdf?id=B1l2bp4YwS), [review](https://openreview.net/forum?id=B1l2bp4YwS)
  - contributions:
    - Describing the expressive power of GNNs via Turing universality.
- **On the Equivalence between Graph Isomorphism Testing and Function Approximation with GNNs.** *Zhengdao Chen, Soledad Villar, Lei Chen, Joan Bruna.* NeurIPS 2019.
  - resources: [paper](https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Review.html), [code](https://github.com/leichen2018/Ring-GNN)
  - contributions:
- **Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology.** *Nima Dehmamy, Albert-László Barabási, Rose Yu.* NeurIPS 2019.
  - resources: [paper](https://papers.nips.cc/paper/2019/file/73bf6c41e241e28b89d0fb9e0c82f9ce-Paper.pdf), [review](https://papers.nips.cc/paper/2019/file/73bf6c41e241e28b89d0fb9e0c82f9ce-Review.html), [code](https://github.com/nimadehmamy/Understanding-GCN)
- **How Powerful are Graph Neural Networks.** *Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka.* ICLR 2019.
  - resouces: [paper](https://openreview.net/pdf?id=ryGs6iA5Km), [review](https://openreview.net/forum?id=ryGs6iA5Km), [code](https://github.com/weihua916/powerful-gnns)
  - contributions:
    - First attempt to analyze the expressive power of GNNs via first order WL-test.
    - Developing GIN.

## Permutation Equivariant GNNs
- **Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks.** *Cristian Bodnar, Fabrizio Frasca, Yu Guang Wang, Nina Otter, Guido Montúfar, Pietro Liò, Michael Bronstein.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2103.03212v1.pdf)
  - contributions:
    - Regard a graph as a special simplical complex and define message passing over simplical complexex with strictly more expressive power than MPNN
- **Isometric Transformation Invariant and Equivariant Graph Convolutional Networks.** *Masanobu Horie, Naoki Morita, Toshiaki Hishinuma, Yu Ihara, Naoto Mitsume.* ICLR 2021.
- **Natural Graph Networks.** *Pim de Haan, Taco Cohen, Max Welling.* NeurIPS 2020.
  - resources: [paper](https://proceedings.neurips.cc/paper/2020/file/2517756c5a9be6ac007fe9bb7fb92611-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/2517756c5a9be6ac007fe9bb7fb92611-Review.html)
  - contributions:
    - General and efficient permutation equivariant GNNs using local equivariance to ensure global equivariance.
    - High level understanding via the category theory. (I don't understand.)
- **Building Powerful and Equivariant Graph Neural Networks with Structural Message-Passing.** *Clement Vignac, Andreas Loukas, Pascal Frossard.* NeurIPS 2020.
  - resources: [paper](https://proceedings.neurips.cc/paper/2020/file/a32d7eeaae19821fd9ce317f3ce952a7-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/a32d7eeaae19821fd9ce317f3ce952a7-Review.html), [code](https://github.com/cvignac/SMP)
  - contributions:
    - Assigning a one-hot identifier vector to each node and propagate it the local context representation of the node.
    - Economic implementation via node coloring.
- **Universal Invariant and Equivariant Graph Neural Networks.** *Nicolas Keriven, Gabriel Peyré.* NeurIPS 2019.
  - resources: [paper](https://proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Reviews.html), [code](https://github.com/nkeriven/univgnn)
  - contributions: 
    - Analyze the expressive power of GNNs by defining a metric space with the edit distance as a metric.
- **Provably Powerful Graph Networks.** *Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman.* NeurIPS 2019.
  - resources: [paper](https://papers.nips.cc/paper/8488-provably-powerful-graph-networks), [review](https://proceedings.neurips.cc/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Reviews.html), [code](https://github.com/hadarser/ProvablyPowerfulGraphNetworks)
  - contributions: 
    - Analyzing the expressive power of GNNs via k order WL test and develop k-WL GNNs.
- **Invariant and Equivariant Graph Networks.** *Haggai Maron, Heli Ben-Hamu, Nadav Shamir, Yaron Lipman.* ICLR 2019.
  - resources: [paper](https://openreview.net/pdf?id=Syx72jC9tm), [review](https://openreview.net/forum?id=Syx72jC9tm), [code](https://github.com/Haggaim/InvariantGraphNetworks)
  - contributions: 
    - Characterizing linear invariant/equivariant mappings by analyzing a linear fixed point equation.
## Novel Message Passing Mechanisms
- **E(n) Equivariant Graph Neural Networks.** *Victor Garcia Satorras, Emiel Hoogeboom, Max Welling.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2102.09844v1.pdf), [code](https://github.com/lucidrains/egnn-pytorch)
  - contributions:
    - Introducing E(n) (rotation, reflection and translation) equivariant GNNs. Useful for trajectory prediction and molecular properties prediction.
- **DeeperGCN: All You Need to Train Deeper GCNs.** *Guohao Li, Chenxin Xiong, Ali Thabet, Bernard Ghanem.* CoRR 2020.
  - resources: [paper](https://arxiv.org/pdf/2006.07739.pdf), [project homepage](https://www.deepgcns.org/)
  - contributions:
    - Generalized aggregation function with mean and max as limits.
    - Message normalization.
    - Limited novelty.
- **Principal Neighbourhood Aggregation for Graph Nets.** *Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, Petar Veličković.* NeurIPS 2020.
  - resources: [paper](https://papers.nips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Review.html), [cdoe](https://github.com/lukecavabarrett/pna)
  - contributions:
    - Normalized moments aggregation.
    - Limited novelty.
## Other Improvements
- **Elastic Graph Neural Networks.** *Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, Jiliang Tang.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2107.06996), [code](https://github.com/lxiaorui/ElasticGNN)
  - contributions:
    - Reformulate popular GNNs as solving the graph signal denoising problem (an optimization problem), introduce l1 / l21 regularization to the optimization problem, and solve them via primal gradient descent (by defining a conjuate function).
- **Graph Neural Networks Inspired by Classical Iterative Algorithms.** *Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng Zhang, Zengfeng Huang, David Wipf.* ICML 2021.
  - resources: [paper](https://arxiv.org/pdf/2103.06064v1)
  - contributions:
    - Reinterpret GNNs (matrix multiplication based) as the solution of an optimization problem with Laplacian regularization.
    - Provide efficient iterative algorithms to find optimals.
    - Competitive or better performance on various types of datasets, even compared with models tailored for specific types of tasks.
    - Provide a principled way to define robust GNNs and avoid oversmoothing.
- **Interpreting and Unifying Graph Neural Networks with An Optimization Framework.** *Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, Peng Cui.* WWW 2021.
  - resources: [paper](https://arxiv.org/pdf/2101.11859v1)
  - contributions:
    - Explicity identify many popular spectral GNNs with multiple layers within an optimization problem with Laplacian regularization.
    - Analyze the expressive power of spectral GNNs by analyzing the coefficients of spectral polynomials.
    - Propose more expressive GNNs by defining optimization problems capturing low-pass / high-pass signals.
- **Identity-aware Graph Neural Networks.** *Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, Jure Leskovec.* AAAI 2021.
  - resources: [paper](https://arxiv.org/pdf/2101.10320v1), [code](https://github.com/snap-stanford/GraphGym)
  - contributions:
    - Differentiate the center node and other nodes during message passing.
    - Able to predict clustering coefficients and shortest path distance.
    - Closely related to *Pan Li et al.*
- **Distance Encoding – Design Provably More Powerful GNNs for Structural Representation Learning.** *Pan Li, Yanbang Wang, Hongwei Wang, Jure Leskovec.* NeurIPS 2020.
  - resources: [paper](https://proceedings.neurips.cc/paper/2020/file/2f73168bf3656f697507752ec592c437-Paper.pdf), [review](https://proceedings.neurips.cc/paper/2020/file/2f73168bf3656f697507752ec592c437-Review.html), [project](http://snap.stanford.edu/distance-encoding/), [code](https://github.com/snap-stanford/distance-encoding)
  - contributions:
    - Pre-computing the distance of any node to a fixed node set and concatenating it to the node attribute.
    - Able to capture the structural role of a node.
    - Able to discriminate non-isomorphic k-regular graphs with high probability.
- **Coloring Graph Neural Networks for Node Disambiguation.** *George Dasoulas, Ludovic Dos Santos, Kevin Scaman, Aladin Virmaux.* IJCAI-PRICA 2020.
  - resources: [paper](https://arxiv.org/pdf/1912.06058.pdf)
  - contributions:
    - Randomly coloring nodes and aggregating information discriminatively.
    - Empirical results are unstable.